#!/usr/bin/env python3
"""
QMOI Performance Optimizer
Monitors and optimizes system performance for maximum efficiency
"""

import os
import sys
import json
import time
import psutil
import asyncio
import logging
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, List, Any, Optional
import gc
import threading

@dataclass
class PerformanceMetrics:
    cpu_usage: float
    memory_usage: float
    disk_usage: float
    network_usage: float
    gpu_usage: Optional[float]
    timestamp: datetime
    process_count: int
    load_average: List[float]

class QMOIPerformanceOptimizer:
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.logs_dir = self.project_root / "logs"
        self.logs_dir.mkdir(exist_ok=True)
        
        # Setup logging
        self.setup_logging()
        
        # Performance thresholds
        self.thresholds = {
            "cpu_warning": 70.0,
            "cpu_critical": 85.0,
            "memory_warning": 75.0,
            "memory_critical": 90.0,
            "disk_warning": 80.0,
            "disk_critical": 95.0
        }
        
        # Optimization strategies
        self.optimization_strategies = self.initialize_optimization_strategies()
        
        # Performance history
        self.performance_history = []
        self.max_history_size = 1000
        
    def setup_logging(self):
        """Setup performance logging"""
        log_file = self.logs_dir / "qmoi-performance.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger("QMOIPerformance")
    
    def initialize_optimization_strategies(self) -> Dict[str, callable]:
        """Initialize optimization strategies"""
        return {
            "memory_optimization": self.optimize_memory,
            "cpu_optimization": self.optimize_cpu,
            "disk_optimization": self.optimize_disk,
            "network_optimization": self.optimize_network,
            "process_optimization": self.optimize_processes,
            "cache_optimization": self.optimize_cache
        }
    
    async def start_performance_monitoring(self):
        """Start continuous performance monitoring"""
        self.logger.info("🚀 Starting QMOI Performance Optimizer")
        self.logger.info("=" * 50)
        
        try:
            while True:
                # Get current performance metrics
                metrics = await self.get_performance_metrics()
                
                # Store metrics in history
                self.store_metrics(metrics)
                
                # Analyze performance
                await self.analyze_performance(metrics)
                
                # Apply optimizations if needed
                await self.apply_optimizations(metrics)
                
                # Generate performance report
                await self.generate_performance_report()
                
                # Wait before next check
                await asyncio.sleep(30)  # Check every 30 seconds
                
        except KeyboardInterrupt:
            self.logger.info("⏹️ Performance monitoring stopped")
        except Exception as e:
            self.logger.error(f"❌ Performance monitoring failed: {e}")
    
    async def get_performance_metrics(self) -> PerformanceMetrics:
        """Get comprehensive performance metrics"""
        try:
            # CPU metrics
            cpu_usage = psutil.cpu_percent(interval=1)
            load_average = psutil.getloadavg()
            
            # Memory metrics
            memory = psutil.virtual_memory()
            memory_usage = memory.percent
            
            # Disk metrics
            disk = psutil.disk_usage('/')
            disk_usage = disk.percent
            
            # Network metrics
            network = psutil.net_io_counters()
            network_usage = (network.bytes_sent + network.bytes_recv) / 1024 / 1024  # MB
            
            # GPU metrics (if available)
            gpu_usage = await self.get_gpu_usage()
            
            # Process count
            process_count = len(psutil.pids())
            
            metrics = PerformanceMetrics(
                cpu_usage=cpu_usage,
                memory_usage=memory_usage,
                disk_usage=disk_usage,
                network_usage=network_usage,
                gpu_usage=gpu_usage,
                timestamp=datetime.now(),
                process_count=process_count,
                load_average=list(load_average)
            )
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"❌ Failed to get performance metrics: {e}")
            # Return default metrics
            return PerformanceMetrics(
                cpu_usage=0.0,
                memory_usage=0.0,
                disk_usage=0.0,
                network_usage=0.0,
                gpu_usage=None,
                timestamp=datetime.now(),
                process_count=0,
                load_average=[0.0, 0.0, 0.0]
            )
    
    async def get_gpu_usage(self) -> Optional[float]:
        """Get GPU usage if available"""
        try:
            # Try to get GPU usage using different methods
            import subprocess
            
            # Try nvidia-smi
            try:
                result = subprocess.run(
                    ["nvidia-smi", "--query-gpu=utilization.gpu", "--format=csv,noheader,nounits"],
                    capture_output=True, text=True, check=True
                )
                gpu_usage = float(result.stdout.strip())
                return gpu_usage
            except (subprocess.CalledProcessError, FileNotFoundError):
                pass
            
            # Try PyTorch if available
            try:
                import torch
                if torch.cuda.is_available():
                    return torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() * 100
            except ImportError:
                pass
            
            return None
            
        except Exception as e:
            self.logger.warning(f"⚠️ Failed to get GPU usage: {e}")
            return None
    
    def store_metrics(self, metrics: PerformanceMetrics):
        """Store metrics in history"""
        self.performance_history.append(metrics)
        
        # Keep history size manageable
        if len(self.performance_history) > self.max_history_size:
            self.performance_history.pop(0)
    
    async def analyze_performance(self, metrics: PerformanceMetrics):
        """Analyze performance and identify issues"""
        try:
            issues = []
            
            # Check CPU usage
            if metrics.cpu_usage > self.thresholds["cpu_critical"]:
                issues.append(("CPU", "CRITICAL", metrics.cpu_usage))
            elif metrics.cpu_usage > self.thresholds["cpu_warning"]:
                issues.append(("CPU", "WARNING", metrics.cpu_usage))
            
            # Check memory usage
            if metrics.memory_usage > self.thresholds["memory_critical"]:
                issues.append(("MEMORY", "CRITICAL", metrics.memory_usage))
            elif metrics.memory_usage > self.thresholds["memory_warning"]:
                issues.append(("MEMORY", "WARNING", metrics.memory_usage))
            
            # Check disk usage
            if metrics.disk_usage > self.thresholds["disk_critical"]:
                issues.append(("DISK", "CRITICAL", metrics.disk_usage))
            elif metrics.disk_usage > self.thresholds["disk_warning"]:
                issues.append(("DISK", "WARNING", metrics.disk_usage))
            
            # Log issues
            for resource, level, usage in issues:
                self.logger.warning(f"⚠️ {resource} {level}: {usage:.1f}%")
            
            # Store analysis results
            analysis = {
                "timestamp": metrics.timestamp.isoformat(),
                "issues": issues,
                "metrics": {
                    "cpu": metrics.cpu_usage,
                    "memory": metrics.memory_usage,
                    "disk": metrics.disk_usage,
                    "network": metrics.network_usage,
                    "gpu": metrics.gpu_usage,
                    "processes": metrics.process_count
                }
            }
            
            # Save analysis
            analysis_file = self.logs_dir / "performance-analysis.json"
            with open(analysis_file, 'a') as f:
                f.write(f"{json.dumps(analysis)}\n")
            
        except Exception as e:
            self.logger.error(f"❌ Performance analysis failed: {e}")
    
    async def apply_optimizations(self, metrics: PerformanceMetrics):
        """Apply optimizations based on current metrics"""
        try:
            optimizations_applied = []
            
            # Memory optimization
            if metrics.memory_usage > self.thresholds["memory_warning"]:
                await self.optimize_memory()
                optimizations_applied.append("memory")
            
            # CPU optimization
            if metrics.cpu_usage > self.thresholds["cpu_warning"]:
                await self.optimize_cpu()
                optimizations_applied.append("cpu")
            
            # Disk optimization
            if metrics.disk_usage > self.thresholds["disk_warning"]:
                await self.optimize_disk()
                optimizations_applied.append("disk")
            
            # Process optimization
            if metrics.process_count > 100:  # High process count
                await self.optimize_processes()
                optimizations_applied.append("processes")
            
            # Cache optimization (periodic)
            if len(self.performance_history) % 10 == 0:  # Every 10 checks
                await self.optimize_cache()
                optimizations_applied.append("cache")
            
            if optimizations_applied:
                self.logger.info(f"🔧 Applied optimizations: {', '.join(optimizations_applied)}")
            
        except Exception as e:
            self.logger.error(f"❌ Optimization application failed: {e}")
    
    async def optimize_memory(self):
        """Optimize memory usage"""
        try:
            self.logger.info("💾 Optimizing memory...")
            
            # Force garbage collection
            gc.collect()
            
            # Clear Python cache
            import importlib
            for module in list(sys.modules.keys()):
                if module.startswith('__'):
                    continue
                try:
                    importlib.reload(sys.modules[module])
                except:
                    pass
            
            # Clear file system cache if possible
            if hasattr(os, 'sync'):
                os.sync()
            
            self.logger.info("✅ Memory optimization completed")
            
        except Exception as e:
            self.logger.error(f"❌ Memory optimization failed: {e}")
    
    async def optimize_cpu(self):
        """Optimize CPU usage"""
        try:
            self.logger.info("⚡ Optimizing CPU...")
            
            # Identify high CPU processes
            high_cpu_processes = []
            for proc in psutil.process_iter(['pid', 'name', 'cpu_percent']):
                try:
                    if proc.info['cpu_percent'] > 10:  # Processes using >10% CPU
                        high_cpu_processes.append(proc.info)
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
            
            # Log high CPU processes
            if high_cpu_processes:
                self.logger.info(f"🔍 High CPU processes: {len(high_cpu_processes)}")
                for proc in high_cpu_processes[:5]:  # Top 5
                    self.logger.info(f"  - {proc['name']}: {proc['cpu_percent']:.1f}%")
            
            # Optimize QMOI processes
            await self.optimize_qmoi_processes()
            
            self.logger.info("✅ CPU optimization completed")
            
        except Exception as e:
            self.logger.error(f"❌ CPU optimization failed: {e}")
    
    async def optimize_qmoi_processes(self):
        """Optimize QMOI-specific processes"""
        try:
            # Find QMOI processes
            qmoi_processes = []
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                try:
                    cmdline = ' '.join(proc.info['cmdline'] or [])
                    if 'qmoi' in cmdline.lower():
                        qmoi_processes.append(proc)
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
            
            # Optimize QMOI processes
            for proc in qmoi_processes:
                try:
                    # Set process priority
                    proc.nice(10)  # Lower priority
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
            
            self.logger.info(f"🔧 Optimized {len(qmoi_processes)} QMOI processes")
            
        except Exception as e:
            self.logger.error(f"❌ QMOI process optimization failed: {e}")
    
    async def optimize_disk(self):
        """Optimize disk usage"""
        try:
            self.logger.info("💿 Optimizing disk...")
            
            # Clean temporary files
            temp_dirs = ["/tmp", "/var/tmp", str(self.project_root / "temp")]
            for temp_dir in temp_dirs:
                if os.path.exists(temp_dir):
                    await self.clean_directory(temp_dir)
            
            # Clean build artifacts
            build_dirs = ["build", "dist", "node_modules/.cache"]
            for build_dir in build_dirs:
                build_path = self.project_root / build_dir
                if build_path.exists():
                    await self.clean_directory(str(build_path))
            
            # Clean logs (keep recent ones)
            await self.clean_old_logs()
            
            self.logger.info("✅ Disk optimization completed")
            
        except Exception as e:
            self.logger.error(f"❌ Disk optimization failed: {e}")
    
    async def clean_directory(self, directory: str):
        """Clean directory of old files"""
        try:
            import shutil
            current_time = time.time()
            max_age = 24 * 60 * 60  # 24 hours
            
            for root, dirs, files in os.walk(directory):
                for file in files:
                    file_path = os.path.join(root, file)
                    try:
                        if os.path.getmtime(file_path) < current_time - max_age:
                            os.remove(file_path)
                    except (OSError, PermissionError):
                        pass
                        
        except Exception as e:
            self.logger.warning(f"⚠️ Failed to clean directory {directory}: {e}")
    
    async def clean_old_logs(self):
        """Clean old log files"""
        try:
            current_time = time.time()
            max_age = 7 * 24 * 60 * 60  # 7 days
            
            for log_file in self.logs_dir.glob("*.log"):
                try:
                    if log_file.stat().st_mtime < current_time - max_age:
                        log_file.unlink()
                except (OSError, PermissionError):
                    pass
                    
        except Exception as e:
            self.logger.warning(f"⚠️ Failed to clean old logs: {e}")
    
    async def optimize_processes(self):
        """Optimize process management"""
        try:
            self.logger.info("🔄 Optimizing processes...")
            
            # Get process information
            processes = []
            for proc in psutil.process_iter(['pid', 'name', 'memory_percent', 'cpu_percent']):
                try:
                    processes.append(proc.info)
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
            
            # Sort by resource usage
            processes.sort(key=lambda x: (x['memory_percent'] or 0) + (x['cpu_percent'] or 0), reverse=True)
            
            # Log top resource users
            self.logger.info("📊 Top resource users:")
            for proc in processes[:5]:
                self.logger.info(f"  - {proc['name']}: CPU {proc['cpu_percent']:.1f}%, MEM {proc['memory_percent']:.1f}%")
            
            # Optimize if too many processes
            if len(processes) > 200:
                await self.reduce_process_count()
            
            self.logger.info("✅ Process optimization completed")
            
        except Exception as e:
            self.logger.error(f"❌ Process optimization failed: {e}")
    
    async def reduce_process_count(self):
        """Reduce process count if too high"""
        try:
            self.logger.info("🔧 Reducing process count...")
            
            # Find and terminate unnecessary processes
            unnecessary_processes = []
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                try:
                    cmdline = ' '.join(proc.info['cmdline'] or [])
                    name = proc.info['name']
                    
                    # Identify unnecessary processes
                    if any(keyword in name.lower() for keyword in ['cache', 'temp', 'tmp']):
                        unnecessary_processes.append(proc)
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
            
            # Terminate unnecessary processes
            for proc in unnecessary_processes[:10]:  # Limit to 10
                try:
                    proc.terminate()
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
            
            self.logger.info(f"🔧 Terminated {len(unnecessary_processes[:10])} unnecessary processes")
            
        except Exception as e:
            self.logger.error(f"❌ Process reduction failed: {e}")
    
    async def optimize_cache(self):
        """Optimize various caches"""
        try:
            self.logger.info("🗂️ Optimizing cache...")
            
            # Clear Python cache
            import importlib.util
            for module_name in list(sys.modules.keys()):
                if module_name.startswith('__'):
                    continue
                try:
                    module = sys.modules[module_name]
                    if hasattr(module, '__file__') and module.__file__:
                        cache_file = module.__file__ + 'c'
                        if os.path.exists(cache_file):
                            os.remove(cache_file)
                except:
                    pass
            
            # Clear npm cache
            try:
                subprocess.run(["npm", "cache", "clean", "--force"], 
                             cwd=self.project_root, check=True)
            except subprocess.CalledProcessError:
                pass
            
            # Clear other caches
            cache_dirs = [".cache", "node_modules/.cache", ".next/cache"]
            for cache_dir in cache_dirs:
                cache_path = self.project_root / cache_dir
                if cache_path.exists():
                    import shutil
                    shutil.rmtree(cache_path)
            
            self.logger.info("✅ Cache optimization completed")
            
        except Exception as e:
            self.logger.error(f"❌ Cache optimization failed: {e}")
    
    async def optimize_network(self):
        """Optimize network usage"""
        try:
            self.logger.info("🌐 Optimizing network...")
            
            # Monitor network connections
            connections = psutil.net_connections()
            
            # Identify high-bandwidth connections
            high_bandwidth = []
            for conn in connections:
                if conn.status == 'ESTABLISHED':
                    # This is a simplified check - in practice you'd need more sophisticated monitoring
                    high_bandwidth.append(conn)
            
            if high_bandwidth:
                self.logger.info(f"🔍 Found {len(high_bandwidth)} active connections")
            
            # Optimize network settings
            # This would include TCP optimization, connection pooling, etc.
            
            self.logger.info("✅ Network optimization completed")
            
        except Exception as e:
            self.logger.error(f"❌ Network optimization failed: {e}")
    
    async def generate_performance_report(self):
        """Generate performance report"""
        try:
            if len(self.performance_history) < 10:  # Need some history
                return
            
            # Calculate averages
            recent_metrics = self.performance_history[-10:]  # Last 10 measurements
            
            avg_cpu = sum(m.cpu_usage for m in recent_metrics) / len(recent_metrics)
            avg_memory = sum(m.memory_usage for m in recent_metrics) / len(recent_metrics)
            avg_disk = sum(m.disk_usage for m in recent_metrics) / len(recent_metrics)
            
            # Generate report
            report = {
                "timestamp": datetime.now().isoformat(),
                "averages": {
                    "cpu": avg_cpu,
                    "memory": avg_memory,
                    "disk": avg_disk
                },
                "current_metrics": {
                    "cpu": recent_metrics[-1].cpu_usage,
                    "memory": recent_metrics[-1].memory_usage,
                    "disk": recent_metrics[-1].disk_usage,
                    "processes": recent_metrics[-1].process_count
                },
                "optimizations_applied": len(self.performance_history) // 10,  # Rough count
                "status": "healthy" if avg_cpu < 70 and avg_memory < 80 else "needs_attention"
            }
            
            # Save report
            report_file = self.logs_dir / "performance-report.json"
            with open(report_file, 'w') as f:
                json.dump(report, f, indent=2)
            
            # Log summary
            self.logger.info(f"📊 Performance Report: CPU {avg_cpu:.1f}%, MEM {avg_memory:.1f}%, DISK {avg_disk:.1f}%")
            
        except Exception as e:
            self.logger.error(f"❌ Performance report generation failed: {e}")
    
    async def get_optimization_recommendations(self) -> List[str]:
        """Get optimization recommendations"""
        try:
            recommendations = []
            
            if len(self.performance_history) < 5:
                return ["Need more performance data for recommendations"]
            
            recent_metrics = self.performance_history[-5:]
            avg_cpu = sum(m.cpu_usage for m in recent_metrics) / len(recent_metrics)
            avg_memory = sum(m.memory_usage for m in recent_metrics) / len(recent_metrics)
            avg_disk = sum(m.disk_usage for m in recent_metrics) / len(recent_metrics)
            
            if avg_cpu > 80:
                recommendations.append("Consider CPU optimization: Reduce concurrent processes")
            
            if avg_memory > 85:
                recommendations.append("Consider memory optimization: Clear caches and unused data")
            
            if avg_disk > 90:
                recommendations.append("Consider disk optimization: Clean temporary files and logs")
            
            if not recommendations:
                recommendations.append("System performance is optimal")
            
            return recommendations
            
        except Exception as e:
            self.logger.error(f"❌ Failed to get optimization recommendations: {e}")
            return ["Unable to generate recommendations"]

async def main():
    """Main entry point"""
    optimizer = QMOIPerformanceOptimizer()
    await optimizer.start_performance_monitoring()

if __name__ == "__main__":
    asyncio.run(main()) 